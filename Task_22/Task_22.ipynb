{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To create a Linear Regression model from scratch, we can follow these steps:\n",
    "\n",
    "2. Initialize Parameters: Start with initial values for weights (w) and bias (b).\n",
    "3. Hypothesis Function: Define the hypothesis function \n",
    "ğ‘¦\n",
    "^\n",
    "=\n",
    "ğ‘¤\n",
    "â‹…\n",
    "ğ‘¥\n",
    "+\n",
    "ğ‘\n",
    "y\n",
    "^\n",
    "â€‹\n",
    " =wâ‹…x+b.\n",
    "4. Loss Function: Use Mean Squared Error (MSE) to measure the performance of the model.\n",
    "5. Gradient Descent: Optimize the parameters by iteratively updating them using the gradient of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialization:\n",
    "2. Hypothesis Function:\n",
    "3. Loss Function:\n",
    "4. Gradient Descent:\n",
    "        Gradients for weights (dw) and bias (db) are computed as:\n",
    "ğ‘‘\n",
    "ğ‘¤\n",
    "=\n",
    "1\n",
    "ğ‘š\n",
    "â‹…\n",
    "(\n",
    "ğ‘‹\n",
    "ğ‘‡\n",
    "â‹…\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘\n",
    "ğ‘Ÿ\n",
    "ğ‘’\n",
    "ğ‘‘\n",
    "âˆ’\n",
    "ğ‘¦\n",
    ")\n",
    ")\n",
    "dw= \n",
    "m\n",
    "1\n",
    "â€‹\n",
    " â‹…(X \n",
    "T\n",
    " â‹…(y \n",
    "pred\n",
    "â€‹\n",
    " âˆ’y))\n",
    "ğ‘‘\n",
    "ğ‘\n",
    "=\n",
    "1\n",
    "ğ‘š\n",
    "â‹…\n",
    "âˆ‘\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘\n",
    "ğ‘Ÿ\n",
    "ğ‘’\n",
    "ğ‘‘\n",
    "âˆ’\n",
    "ğ‘¦\n",
    ")\n",
    "db= \n",
    "m\n",
    "1\n",
    "â€‹\n",
    " â‹…âˆ‘(y \n",
    "pred\n",
    "â€‹\n",
    " âˆ’y) \n",
    "      Parameters are updated using the learning rate:\n",
    "self.w -= self.learning_rate * dw\n",
    "self.b -= self.learning_rate * db\n",
    "5. Training and Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize parameters\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.b = 0\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Gradient Descent\n",
    "        for i in range(self.iterations):\n",
    "            # Make predictions\n",
    "            y_pred = np.dot(X, self.w) + self.b\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/m) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.learning_rate * dw\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            # Print loss every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                loss = (1/m) * np.sum((y_pred - y) ** 2)\n",
    "                print(f'Iteration {i}: Loss = {loss}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 75.5\n",
      "Iteration 100: Loss = 0.2900145764980926\n",
      "Iteration 200: Loss = 0.24756005236129197\n",
      "Iteration 300: Loss = 0.21150076070797114\n",
      "Iteration 400: Loss = 0.18083662105405643\n",
      "Iteration 500: Loss = 0.15473420529682097\n",
      "Iteration 600: Loss = 0.1324935148081967\n",
      "Iteration 700: Loss = 0.11352580810583981\n",
      "Iteration 800: Loss = 0.09733524833649738\n",
      "Iteration 900: Loss = 0.08350369071904144\n",
      "Predictions: [ 5.63276732  7.72023088  9.14302565 11.23048921]\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "    y = np.dot(X, np.array([1, 2])) + 3\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression(learning_rate=0.01, iterations=1000)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
